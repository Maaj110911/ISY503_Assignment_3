{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e63dfee7",
   "metadata": {},
   "source": [
    "# Sentiment ANalysis on Amazon Product Reviews Dataset #\n",
    "\n",
    "Sourced from: Blitzer, J., Dredze, M., & Pereira, F. (2007). Biographies, Bollywood, Boom-boxes and Blenders: Domain adaptation for sentiment classification. Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, 440â€“447. http://www.cs.jhu.edu/~mdredze/datasets/sentiment/index2.html\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b1daecdf",
   "metadata": {},
   "source": [
    "## Importing the Required Libraries ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32fa9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.classifier_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Downloading necessary NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "75d6f447",
   "metadata": {},
   "source": [
    "## Loading the Dataset ##\n",
    "The dataset was loaded using the html parser from Beautiful Soup. The code was compiled with step by step assitance from various websites and articles. \n",
    "\n",
    "Assistance taken from:\n",
    "\n",
    "https://oxylabs.io/blog/beautiful-soup-parsing-tutorial\n",
    "\n",
    "https://stackoverflow.com/questions/21570780/using-python-and-beautifulsoup-saved-webpage-source-codes-into-a-local-file\n",
    "\n",
    "https://stackoverflow.com/questions/43214305/how-to-use-text-strip-function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "baaeb4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'sorted_data_acl'\n",
    "categories = ['books', 'dvd', 'electronics', 'kitchen_&_housewares']\n",
    "data = []\n",
    "\n",
    "for category in categories:\n",
    "    for file in ['negative', 'positive']:\n",
    "        path = os.path.join(folder, category, f\"{file}.review\")\n",
    "        with open(path, 'r', encoding='utf-8') as file:\n",
    "            soup = BeautifulSoup(file, 'html.parser')\n",
    "            reviews = soup.find_all('review_text')\n",
    "            \n",
    "            for review in reviews:\n",
    "                clean_text = review.text.strip()  # Removing  leading amd trailing whitespace,  assistance taken from https://stackoverflow.com/questions/43214305/how-to-use-text-strip-function\n",
    "                data.append((clean_text, 1 if file == 'positive' else 0))\n",
    "\n",
    "df = pd.DataFrame(data, columns=['review_text', 'file'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "853e8341",
   "metadata": {},
   "source": [
    "## Viewing the Dataframe ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "867ca4d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>THis book was horrible.  If it was possible to...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I like to use the Amazon reviews when purchasi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>THis book was horrible.  If it was possible to...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm not sure who's writing these reviews, but ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I picked up the first book in this series (The...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_text  sentiment\n",
       "0  THis book was horrible.  If it was possible to...          0\n",
       "1  I like to use the Amazon reviews when purchasi...          0\n",
       "2  THis book was horrible.  If it was possible to...          0\n",
       "3  I'm not sure who's writing these reviews, but ...          0\n",
       "4  I picked up the first book in this series (The...          0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "465e6ec2",
   "metadata": {},
   "source": [
    "## Removing Outliers: Very short reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed997c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function for counting words\n",
    "def word_count(text):\n",
    "    return len(text.split())\n",
    "\n",
    "\n",
    "#Assistance taken from https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html\n",
    "min_length = 10 \n",
    "\n",
    "df['word_count'] = df['review_text'].apply(word_count)\n",
    "df = df[df['word_count'] >= min_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5be70909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive reviews: 4000\n",
      "Number of negative reviews: 4000\n"
     ]
    }
   ],
   "source": [
    "# Counting the number of positive and negative reviews\n",
    "positive_count = df[df['file'] == 1].shape[0]\n",
    "negative_count = df[df['file'] == 0].shape[0]\n",
    "\n",
    "print(f\"Number of positive reviews: {positive_count}\")\n",
    "print(f\"Number of negative reviews: {negative_count}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dfbb4704",
   "metadata": {},
   "source": [
    "## Pre-processing the data ##\n",
    "\n",
    "The pre-processing task was done with asistance from:\n",
    "\n",
    "https://www.dataquest.io/blog/how-to-clean-and-prepare-your-data-for-analysis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8eaf34b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the text\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize\n",
    "    words = nltk.word_tokenize(text.lower())\n",
    "    # Remove stopwords and stem\n",
    "    filtered_words = [ps.stem(word) for word in words if word not in stop_words and word.isalpha()]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "df['processed_text'] = df['review_text'].apply(preprocess_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f459dd3",
   "metadata": {},
   "source": [
    "### Vectorization of processed data ###\n",
    "\n",
    "Assistance from https://medium.com/@WojtekFulmyk/text-tokenization-and-vectorization-in-nlp-ac5e3eb35b85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fbeb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df['processed_text'])\n",
    "y = df['file']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "012d7ef9",
   "metadata": {},
   "source": [
    "### Spliting the Data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12a61679",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "64f8ef29",
   "metadata": {},
   "source": [
    "## Building the Logistic Regression classifier and training ##\n",
    "\n",
    "Assistance from : https://spotintelligence.com/2023/02/22/logistic-regression-text-classification-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "970de7ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_classifier import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression(max_iter=1000)  \n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4de25b9f",
   "metadata": {},
   "source": [
    "### Caluclating Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bde4ed56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.815625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "05b2ee8e",
   "metadata": {},
   "source": [
    "## Classifier Validation ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c6709664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_file(review):\n",
    "    processed_review = preprocess_text(review)\n",
    "    vectorized_review = vectorizer.transform([processed_review])\n",
    "    prediction = classifier.predict(vectorized_review)\n",
    "    return \"Positive\" if prediction[0] == 1 else \"Negative\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2aa87692",
   "metadata": {},
   "source": [
    "### Testing with a complicated Negative Review ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5c78a7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "print(predict_file(\"I don't know what to say about this product. The quality of paper was super, and the fininsh just right, but then again the glue used laid waste to it all. All beautiful things broken apart and scattered around\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27898253",
   "metadata": {},
   "source": [
    "### Testing with a complicated Positive Review ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ecd861be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test the function\n",
    "print(predict_file(\"I don't know what to say about this product. The quality of paper was super, and the fininsh just right\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0ce8f7b",
   "metadata": {},
   "source": [
    "*End of Code*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
