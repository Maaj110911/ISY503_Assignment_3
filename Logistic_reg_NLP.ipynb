{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e63dfee7",
   "metadata": {},
   "source": [
    "# Sentiment ANalysis on Amazon Product Reviews Dataset #\n",
    "\n",
    "Sourced from: Blitzer, J., Dredze, M., & Pereira, F. (2007). Biographies, Bollywood, Boom-boxes and Blenders: Domain adaptation for sentiment classification. Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, 440â€“447. http://www.cs.jhu.edu/~mdredze/datasets/sentiment/index2.html\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b1daecdf",
   "metadata": {},
   "source": [
    "## Importing the Required Libraries ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32fa9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.classifier_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Downloading necessary NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "75d6f447",
   "metadata": {},
   "source": [
    "## Loading the Dataset ##\n",
    "The dataset was loaded using the html parser from Beautiful Soup. The code was compiled with step by step assitance from various websites and articles. \n",
    "\n",
    "Assistance taken from:\n",
    "\n",
    "https://oxylabs.io/blog/beautiful-soup-parsing-tutorial\n",
    "\n",
    "https://stackoverflow.com/questions/21570780/using-python-and-beautifulsoup-saved-webpage-source-codes-into-a-local-file\n",
    "\n",
    "https://stackoverflow.com/questions/43214305/how-to-use-text-strip-function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "baaeb4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'sorted_data_acl'\n",
    "categories = ['books', 'dvd', 'electronics', 'kitchen_&_housewares']\n",
    "data = []\n",
    "\n",
    "for category in categories:\n",
    "    for file in ['negative', 'positive']:\n",
    "        path = os.path.join(folder, category, f\"{file}.review\")\n",
    "        with open(path, 'r', encoding='utf-8') as file:\n",
    "            soup = BeautifulSoup(file, 'html.parser')\n",
    "            reviews = soup.find_all('review_text')\n",
    "            \n",
    "            for review in reviews:\n",
    "                clean_text = review.text.strip()  # Removing  leading amd trailing whitespace,  assistance taken from https://stackoverflow.com/questions/43214305/how-to-use-text-strip-function\n",
    "                data.append((clean_text, 1 if file == 'positive' else 0))\n",
    "\n",
    "df = pd.DataFrame(data, columns=['review_text', 'file'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "853e8341",
   "metadata": {},
   "source": [
    "## Viewing the Dataframe ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "867ca4d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>THis book was horrible.  If it was possible to...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I like to use the Amazon reviews when purchasi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>THis book was horrible.  If it was possible to...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm not sure who's writing these reviews, but ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I picked up the first book in this series (The...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_text  sentiment\n",
       "0  THis book was horrible.  If it was possible to...          0\n",
       "1  I like to use the Amazon reviews when purchasi...          0\n",
       "2  THis book was horrible.  If it was possible to...          0\n",
       "3  I'm not sure who's writing these reviews, but ...          0\n",
       "4  I picked up the first book in this series (The...          0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "465e6ec2",
   "metadata": {},
   "source": [
    "## Removing Outliers: Very short reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed997c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function for counting words\n",
    "def word_count(text):\n",
    "    return len(text.split())\n",
    "\n",
    "\n",
    "#Assistance taken from https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html\n",
    "min_length = 10 \n",
    "\n",
    "df['word_count'] = df['review_text'].apply(word_count)\n",
    "df = df[df['word_count'] >= min_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5be70909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive reviews: 4000\n",
      "Number of negative reviews: 4000\n"
     ]
    }
   ],
   "source": [
    "# Counting the number of positive and negative reviews\n",
    "positive_count = df[df['file'] == 1].shape[0]\n",
    "negative_count = df[df['file'] == 0].shape[0]\n",
    "\n",
    "print(f\"Number of positive reviews: {positive_count}\")\n",
    "print(f\"Number of negative reviews: {negative_count}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dfbb4704",
   "metadata": {},
   "source": [
    "## Pre-processing the data ##\n",
    "\n",
    "The pre-processing task was done with asistance from:\n",
    "\n",
    "https://www.dataquest.io/blog/how-to-clean-and-prepare-your-data-for-analysis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8eaf34b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the text\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize\n",
    "    words = nltk.word_tokenize(text.lower())\n",
    "    # Remove stopwords and stem\n",
    "    filtered_words = [ps.stem(word) for word in words if word not in stop_words and word.isalpha()]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "df['processed_text'] = df['review_text'].apply(preprocess_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f459dd3",
   "metadata": {},
   "source": [
    "### Vectorization of processed data ###\n",
    "\n",
    "Assistance from https://medium.com/@WojtekFulmyk/text-tokenization-and-vectorization-in-nlp-ac5e3eb35b85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fbeb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df['processed_text'])\n",
    "y = df['file']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "012d7ef9",
   "metadata": {},
   "source": [
    "### Spliting the Data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12a61679",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "64f8ef29",
   "metadata": {},
   "source": [
    "## Building the Logistic Regression classifier and training ##\n",
    "\n",
    "Assistance from : https://spotintelligence.com/2023/02/22/logistic-regression-text-classification-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "970de7ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_classifier import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression(max_iter=1000)  \n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4de25b9f",
   "metadata": {},
   "source": [
    "### Caluclating Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bde4ed56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.815625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "05b2ee8e",
   "metadata": {},
   "source": [
    "## Classifier Validation ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c6709664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_file(review):\n",
    "    processed_review = preprocess_text(review)\n",
    "    vectorized_review = vectorizer.transform([processed_review])\n",
    "    prediction = classifier.predict(vectorized_review)\n",
    "    return \"Positive\" if prediction[0] == 1 else \"Negative\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2aa87692",
   "metadata": {},
   "source": [
    "### Testing with a complicated Negative Review ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5c78a7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "print(predict_file(\"I don't know what to say about this product. The quality of paper was super, and the fininsh just right, but then again the glue used laid waste to it all. All beautiful things broken apart and scattered around\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27898253",
   "metadata": {},
   "source": [
    "### Testing with a complicated Positive Review ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ecd861be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test the function\n",
    "print(predict_file(\"I don't know what to say about this product. The quality of paper was super, and the fininsh just right\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0ce8f7b",
   "metadata": {},
   "source": [
    "*End of Code*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ad2078c",
   "metadata": {},
   "source": [
    "# Continuing Further model Exploration #"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e18e4e1c",
   "metadata": {},
   "source": [
    "## Support Vector Machine - SVM ##\n",
    "\n",
    "Assistance from: https://scikit-learn.org/stable/modules/svm.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cd4c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear', probability=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Creating the SVM model\n",
    "SVC_model = SVC(kernel='linear', probability=True)\n",
    "\n",
    "# Training the model\n",
    "SVC_model.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "35be8b1c",
   "metadata": {},
   "source": [
    "### Checking Model Performance ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e693496b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.776875\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3785ddb4",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier ##\n",
    "\n",
    "Assistance from: https://scikit-learn.org/stable/modules/naive_bayes.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a3ee77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Creating and training the Naive Bayes classifier\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae7ed21d",
   "metadata": {},
   "source": [
    "### Checking Model Performance ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5b8d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.789375\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3401f9d1",
   "metadata": {},
   "source": [
    "## Decision Tree ##\n",
    "\n",
    "Assistance from: https://scikit-learn.org/stable/modules/tree.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0de5eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.688125\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "dt_model = DecisionTreeClassifier()\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = dt_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8e1fd5c9",
   "metadata": {},
   "source": [
    "## Random Forest ##\n",
    "\n",
    "Assistance from: https://scikit-learn.org/stable/modules/ensemble.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8307915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100)  \n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72477ee2",
   "metadata": {},
   "source": [
    "### Checking Model Performance ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7788a07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.81125\n"
     ]
    }
   ],
   "source": [
    "y_pred = rf_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2bfed1a",
   "metadata": {},
   "source": [
    "## Gradient Boosting ##\n",
    "\n",
    "Assistance from: https://scikit-learn.org/stable/modules/ensemble.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a687555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbm_model = GradientBoostingClassifier()\n",
    "gbm_model.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d494569a",
   "metadata": {},
   "source": [
    "### Checking Model Performance ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d909c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.775\n"
     ]
    }
   ],
   "source": [
    "y_pred = gbm_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "347305b7",
   "metadata": {},
   "source": [
    "## XG Boost Model ##\n",
    "\n",
    "Assistance from: https://scikit-learn.org/stable/modules/ensemble.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac9e2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Keshav\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric='mlogloss',\n",
       "              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
       "              num_parallel_tree=None, predictor=None, random_state=None, ...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "xgb_model.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4227f27d",
   "metadata": {},
   "source": [
    "### Checking Model Performance ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1215a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.805625\n"
     ]
    }
   ],
   "source": [
    "y_pred = xgb_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23a549d8",
   "metadata": {},
   "source": [
    "## Ensmeble Model ##\n",
    "\n",
    "Assitance from : https://scikit-learn.org/stable/modules/ensemble.html\n",
    "\n",
    "https://machinelearningmastery.com/voting-ensembles-with-python/\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c50ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "model1 = LogisticRegression(max_iter=1000)\n",
    "model2 = RandomForestClassifier(n_estimators=100)\n",
    "model3 = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "391058e7",
   "metadata": {},
   "source": [
    "### Training the Model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20436b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "ensemble = VotingClassifier(estimators=[\n",
    "    ('lr', model1), \n",
    "    ('rf', model2), \n",
    "    ('xgb', model3)\n",
    "], voting='soft')\n",
    "\n",
    "ensemble.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a115ee1",
   "metadata": {},
   "source": [
    "### Checking Model Performance ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a87a531",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = ensemble.predict(X_test) \n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Ensemble Model Accuracy:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c019e28",
   "metadata": {},
   "source": [
    "# Building the Neural Network Model #"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0797432d",
   "metadata": {},
   "source": [
    "Assistance from: https://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l09c04_nlp_embeddings_and_sentiment.ipynb\n",
    "\n",
    "https://www.kaggle.com/code/dilipkumar2k6/tensorflow-nlp-word-embedding-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc68dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25732367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "vocab_size = 10000\n",
    "max_length = 100\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "oov_tok = '<OOV>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04d384c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = df['review_text'].tolist()\n",
    "labels = df['sentiment'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f992b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(reviews)\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(reviews)\n",
    "padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202755b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data\n",
    "training_size = int(len(reviews) * 0.7)\n",
    "train_padded = padded[:training_size]\n",
    "test_padded = padded[training_size:]\n",
    "train_labels = labels[:training_size]\n",
    "test_labels = labels[training_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972c0a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Converting labels to numpy arrays\n",
    "train_labels = np.array(train_labels).astype('float32')\n",
    "test_labels = np.array(test_labels).astype('float32')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42e69a3a",
   "metadata": {},
   "source": [
    "## Model Architecture ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8d3a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 100, 16)           160000    \n",
      "                                                                 \n",
      " global_average_pooling1d_2   (None, 16)               0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dropout_39 (Dropout)        (None, 16)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 12)                204       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 13        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160,217\n",
      "Trainable params: 160,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, 16, input_length=max_length),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dropout(0.5),  # Adding dropout\n",
    "    tf.keras.layers.Dense(12, activation='elu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),  # L2 regularization\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1dc68023",
   "metadata": {},
   "source": [
    "## Model Training ##\n",
    "\n",
    "The model training was stopped midway as the mdoel started to overfit, the validation loss value started increasing.\n",
    "\n",
    "Assistance from: https://www.tensorflow.org/api_docs/python/tf/data/Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4993ed9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "175/175 - 4s - loss: 0.8064 - accuracy: 0.5309 - val_loss: 0.7579 - val_accuracy: 0.4167 - 4s/epoch - 20ms/step\n",
      "Epoch 2/100\n",
      "175/175 - 1s - loss: 0.7354 - accuracy: 0.5352 - val_loss: 0.7191 - val_accuracy: 0.4167 - 1s/epoch - 7ms/step\n",
      "Epoch 3/100\n",
      "175/175 - 1s - loss: 0.7110 - accuracy: 0.5339 - val_loss: 0.7059 - val_accuracy: 0.4167 - 1s/epoch - 7ms/step\n",
      "Epoch 4/100\n",
      "175/175 - 1s - loss: 0.7023 - accuracy: 0.5298 - val_loss: 0.7012 - val_accuracy: 0.4167 - 1s/epoch - 7ms/step\n",
      "Epoch 5/100\n",
      "175/175 - 1s - loss: 0.6984 - accuracy: 0.5250 - val_loss: 0.6984 - val_accuracy: 0.4671 - 1s/epoch - 6ms/step\n",
      "Epoch 6/100\n",
      "175/175 - 1s - loss: 0.6949 - accuracy: 0.5314 - val_loss: 0.6950 - val_accuracy: 0.5350 - 1s/epoch - 6ms/step\n",
      "Epoch 7/100\n",
      "175/175 - 1s - loss: 0.6899 - accuracy: 0.5539 - val_loss: 0.6905 - val_accuracy: 0.5671 - 1s/epoch - 7ms/step\n",
      "Epoch 8/100\n",
      "175/175 - 1s - loss: 0.6823 - accuracy: 0.5832 - val_loss: 0.6842 - val_accuracy: 0.5967 - 1s/epoch - 7ms/step\n",
      "Epoch 9/100\n",
      "175/175 - 1s - loss: 0.6712 - accuracy: 0.6250 - val_loss: 0.6749 - val_accuracy: 0.6271 - 1s/epoch - 7ms/step\n",
      "Epoch 10/100\n",
      "175/175 - 1s - loss: 0.6522 - accuracy: 0.6714 - val_loss: 0.6606 - val_accuracy: 0.6679 - 1s/epoch - 6ms/step\n",
      "Epoch 11/100\n",
      "175/175 - 1s - loss: 0.6258 - accuracy: 0.7088 - val_loss: 0.6391 - val_accuracy: 0.7075 - 1s/epoch - 7ms/step\n",
      "Epoch 12/100\n",
      "175/175 - 1s - loss: 0.5993 - accuracy: 0.7418 - val_loss: 0.6172 - val_accuracy: 0.7371 - 1s/epoch - 6ms/step\n",
      "Epoch 13/100\n",
      "175/175 - 1s - loss: 0.5682 - accuracy: 0.7661 - val_loss: 0.5981 - val_accuracy: 0.7500 - 1s/epoch - 7ms/step\n",
      "Epoch 14/100\n",
      "175/175 - 1s - loss: 0.5372 - accuracy: 0.7859 - val_loss: 0.5812 - val_accuracy: 0.7633 - 1s/epoch - 7ms/step\n",
      "Epoch 15/100\n",
      "175/175 - 1s - loss: 0.5102 - accuracy: 0.8055 - val_loss: 0.5682 - val_accuracy: 0.7667 - 1s/epoch - 7ms/step\n",
      "Epoch 16/100\n",
      "175/175 - 1s - loss: 0.4854 - accuracy: 0.8223 - val_loss: 0.5578 - val_accuracy: 0.7692 - 1s/epoch - 7ms/step\n",
      "Epoch 17/100\n",
      "175/175 - 1s - loss: 0.4580 - accuracy: 0.8393 - val_loss: 0.5490 - val_accuracy: 0.7692 - 1s/epoch - 6ms/step\n",
      "Epoch 18/100\n",
      "175/175 - 1s - loss: 0.4387 - accuracy: 0.8504 - val_loss: 0.5434 - val_accuracy: 0.7746 - 1s/epoch - 7ms/step\n",
      "Epoch 19/100\n",
      "175/175 - 1s - loss: 0.4137 - accuracy: 0.8655 - val_loss: 0.5387 - val_accuracy: 0.7767 - 1s/epoch - 7ms/step\n",
      "Epoch 20/100\n",
      "175/175 - 1s - loss: 0.3975 - accuracy: 0.8736 - val_loss: 0.5354 - val_accuracy: 0.7767 - 1s/epoch - 7ms/step\n",
      "Epoch 21/100\n",
      "175/175 - 1s - loss: 0.3793 - accuracy: 0.8848 - val_loss: 0.5330 - val_accuracy: 0.7758 - 1s/epoch - 6ms/step\n",
      "Epoch 22/100\n",
      "175/175 - 1s - loss: 0.3611 - accuracy: 0.8913 - val_loss: 0.5319 - val_accuracy: 0.7754 - 1s/epoch - 7ms/step\n",
      "Epoch 23/100\n",
      "175/175 - 1s - loss: 0.3513 - accuracy: 0.9011 - val_loss: 0.5304 - val_accuracy: 0.7742 - 1s/epoch - 6ms/step\n",
      "Epoch 24/100\n",
      "175/175 - 1s - loss: 0.3369 - accuracy: 0.9046 - val_loss: 0.5314 - val_accuracy: 0.7763 - 1s/epoch - 7ms/step\n",
      "Epoch 25/100\n",
      "175/175 - 1s - loss: 0.3239 - accuracy: 0.9121 - val_loss: 0.5324 - val_accuracy: 0.7763 - 1s/epoch - 7ms/step\n",
      "Epoch 26/100\n",
      "175/175 - 1s - loss: 0.3090 - accuracy: 0.9177 - val_loss: 0.5358 - val_accuracy: 0.7775 - 1s/epoch - 7ms/step\n",
      "Epoch 27/100\n",
      "175/175 - 1s - loss: 0.3011 - accuracy: 0.9202 - val_loss: 0.5375 - val_accuracy: 0.7792 - 1s/epoch - 6ms/step\n",
      "Epoch 28/100\n",
      "175/175 - 1s - loss: 0.2859 - accuracy: 0.9255 - val_loss: 0.5380 - val_accuracy: 0.7792 - 1s/epoch - 6ms/step\n",
      "Epoch 29/100\n",
      "175/175 - 1s - loss: 0.2801 - accuracy: 0.9302 - val_loss: 0.5400 - val_accuracy: 0.7775 - 1s/epoch - 7ms/step\n",
      "Epoch 30/100\n",
      "175/175 - 1s - loss: 0.2686 - accuracy: 0.9332 - val_loss: 0.5464 - val_accuracy: 0.7792 - 1s/epoch - 7ms/step\n",
      "Epoch 31/100\n",
      "175/175 - 1s - loss: 0.2615 - accuracy: 0.9373 - val_loss: 0.5499 - val_accuracy: 0.7796 - 1s/epoch - 6ms/step\n",
      "Epoch 32/100\n",
      "175/175 - 1s - loss: 0.2508 - accuracy: 0.9405 - val_loss: 0.5529 - val_accuracy: 0.7796 - 1s/epoch - 7ms/step\n",
      "Epoch 33/100\n",
      "175/175 - 1s - loss: 0.2434 - accuracy: 0.9436 - val_loss: 0.5571 - val_accuracy: 0.7800 - 1s/epoch - 7ms/step\n",
      "Epoch 34/100\n",
      "175/175 - 1s - loss: 0.2338 - accuracy: 0.9486 - val_loss: 0.5638 - val_accuracy: 0.7783 - 1s/epoch - 6ms/step\n",
      "Epoch 35/100\n",
      "175/175 - 1s - loss: 0.2270 - accuracy: 0.9505 - val_loss: 0.5662 - val_accuracy: 0.7796 - 1s/epoch - 7ms/step\n",
      "Epoch 36/100\n",
      "175/175 - 1s - loss: 0.2197 - accuracy: 0.9543 - val_loss: 0.5736 - val_accuracy: 0.7788 - 1s/epoch - 7ms/step\n",
      "Epoch 37/100\n",
      "175/175 - 1s - loss: 0.2136 - accuracy: 0.9557 - val_loss: 0.5745 - val_accuracy: 0.7796 - 1s/epoch - 7ms/step\n",
      "Epoch 38/100\n",
      "175/175 - 1s - loss: 0.2043 - accuracy: 0.9582 - val_loss: 0.5853 - val_accuracy: 0.7763 - 1s/epoch - 6ms/step\n",
      "Epoch 39/100\n",
      "175/175 - 1s - loss: 0.2015 - accuracy: 0.9607 - val_loss: 0.5878 - val_accuracy: 0.7779 - 1s/epoch - 6ms/step\n",
      "Epoch 40/100\n",
      "175/175 - 1s - loss: 0.1947 - accuracy: 0.9607 - val_loss: 0.5956 - val_accuracy: 0.7788 - 1s/epoch - 6ms/step\n",
      "Epoch 41/100\n",
      "175/175 - 1s - loss: 0.1882 - accuracy: 0.9654 - val_loss: 0.6045 - val_accuracy: 0.7779 - 1s/epoch - 7ms/step\n",
      "Epoch 42/100\n",
      "175/175 - 1s - loss: 0.1832 - accuracy: 0.9663 - val_loss: 0.6041 - val_accuracy: 0.7754 - 1s/epoch - 7ms/step\n",
      "Epoch 43/100\n",
      "175/175 - 1s - loss: 0.1771 - accuracy: 0.9655 - val_loss: 0.6140 - val_accuracy: 0.7750 - 1s/epoch - 6ms/step\n",
      "Epoch 44/100\n",
      "175/175 - 1s - loss: 0.1733 - accuracy: 0.9709 - val_loss: 0.6222 - val_accuracy: 0.7733 - 1s/epoch - 7ms/step\n",
      "Epoch 45/100\n",
      "175/175 - 1s - loss: 0.1681 - accuracy: 0.9695 - val_loss: 0.6311 - val_accuracy: 0.7746 - 1s/epoch - 7ms/step\n",
      "Epoch 46/100\n",
      "175/175 - 1s - loss: 0.1619 - accuracy: 0.9736 - val_loss: 0.6341 - val_accuracy: 0.7738 - 1s/epoch - 6ms/step\n",
      "Epoch 47/100\n",
      "175/175 - 1s - loss: 0.1570 - accuracy: 0.9748 - val_loss: 0.6424 - val_accuracy: 0.7750 - 1s/epoch - 6ms/step\n",
      "Epoch 48/100\n",
      "175/175 - 1s - loss: 0.1535 - accuracy: 0.9766 - val_loss: 0.6521 - val_accuracy: 0.7738 - 1s/epoch - 6ms/step\n",
      "Epoch 49/100\n",
      "175/175 - 1s - loss: 0.1504 - accuracy: 0.9766 - val_loss: 0.6541 - val_accuracy: 0.7729 - 1s/epoch - 6ms/step\n",
      "Epoch 50/100\n",
      "175/175 - 1s - loss: 0.1478 - accuracy: 0.9800 - val_loss: 0.6637 - val_accuracy: 0.7721 - 1s/epoch - 7ms/step\n",
      "Epoch 51/100\n",
      "175/175 - 1s - loss: 0.1425 - accuracy: 0.9789 - val_loss: 0.6712 - val_accuracy: 0.7713 - 1s/epoch - 6ms/step\n",
      "Epoch 52/100\n",
      "175/175 - 1s - loss: 0.1383 - accuracy: 0.9796 - val_loss: 0.6814 - val_accuracy: 0.7713 - 1s/epoch - 7ms/step\n",
      "Epoch 53/100\n",
      "175/175 - 1s - loss: 0.1326 - accuracy: 0.9829 - val_loss: 0.6856 - val_accuracy: 0.7696 - 1s/epoch - 7ms/step\n",
      "Epoch 54/100\n",
      "175/175 - 1s - loss: 0.1316 - accuracy: 0.9820 - val_loss: 0.6940 - val_accuracy: 0.7692 - 1s/epoch - 6ms/step\n",
      "Epoch 55/100\n",
      "175/175 - 1s - loss: 0.1291 - accuracy: 0.9841 - val_loss: 0.7037 - val_accuracy: 0.7700 - 1s/epoch - 7ms/step\n",
      "Epoch 56/100\n",
      "175/175 - 1s - loss: 0.1244 - accuracy: 0.9843 - val_loss: 0.7042 - val_accuracy: 0.7692 - 1s/epoch - 7ms/step\n",
      "Epoch 57/100\n",
      "175/175 - 1s - loss: 0.1207 - accuracy: 0.9848 - val_loss: 0.7209 - val_accuracy: 0.7679 - 1s/epoch - 6ms/step\n",
      "Epoch 58/100\n",
      "175/175 - 1s - loss: 0.1202 - accuracy: 0.9848 - val_loss: 0.7219 - val_accuracy: 0.7658 - 1s/epoch - 6ms/step\n",
      "Epoch 59/100\n",
      "175/175 - 1s - loss: 0.1165 - accuracy: 0.9862 - val_loss: 0.7349 - val_accuracy: 0.7671 - 1s/epoch - 6ms/step\n",
      "Epoch 60/100\n",
      "175/175 - 1s - loss: 0.1107 - accuracy: 0.9887 - val_loss: 0.7376 - val_accuracy: 0.7671 - 1s/epoch - 7ms/step\n",
      "Epoch 61/100\n",
      "175/175 - 1s - loss: 0.1092 - accuracy: 0.9879 - val_loss: 0.7470 - val_accuracy: 0.7658 - 1s/epoch - 7ms/step\n",
      "Epoch 62/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6888\\1034932968.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m      7\u001b[0m \u001b[1;31m# Now use the dataset objects for training and evaluation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m----> 8\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n",
      "\u001b[0;32m   1648\u001b[0m                         ):\n",
      "\u001b[0;32m   1649\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m-> 1650\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   1651\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m   1652\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n",
      "\u001b[0;32m    878\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    879\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n",
      "\u001b[0;32m    910\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    911\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m--> 912\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    913\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    914\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    132\u001b[0m       (concrete_function,\n",
      "\u001b[0;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n",
      "\u001b[1;32m--> 134\u001b[1;33m     return concrete_function._call_flat(\n",
      "\u001b[0m\u001b[0;32m    135\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n",
      "\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n",
      "\u001b[0;32m   1743\u001b[0m         and executing_eagerly):\n",
      "\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m-> 1745\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n",
      "\u001b[0m\u001b[0;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n",
      "\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n",
      "\u001b[0;32m    376\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m--> 378\u001b[1;33m           outputs = execute.execute(\n",
      "\u001b[0m\u001b[0;32m    379\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    380\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n",
      "\u001b[0;32m     50\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m---> 52\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
      "\u001b[0m\u001b[0;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n",
      "\u001b[0;32m     54\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_padded, train_labels))\n",
    "train_dataset = train_dataset.batch(32)  # You can adjust the batch size\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_padded, test_labels))\n",
    "test_dataset = test_dataset.batch(32)\n",
    "\n",
    "# Now use the dataset objects for training and evaluation\n",
    "model.fit(train_dataset, epochs=100, validation_data=test_dataset, verbose=2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c62a7676",
   "metadata": {},
   "source": [
    "# Saving the Best Model for Executable File #\n",
    "\n",
    "Assistance from: https://medium.com/@maziarizadi/pickle-your-model-in-python-2bbe7dba2bbb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e09b412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('logistic_regression_model.pkl', 'wb') as file:\n",
    "    pickle.dump(classifier, file)\n",
    "with open('vectorizer.pkl', 'wb') as file:\n",
    "    pickle.dump(vectorizer, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
